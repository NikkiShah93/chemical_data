{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praedicat assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment I have four data sets that each includes different properties of chemical compound.\n",
    "The goal is to have a database that contains name, CAS number, the source/s and some properties that are optional.\n",
    "The four data sets are:\n",
    "1.\tChemIDPlus.  This is stored in XML format.  \n",
    "2.\tMeSH Supplemental Records.  This is stored in XML format.\n",
    "3.\tPesticide Product Information System (PPIS).  This is in text table format across multiple files. \n",
    "4.\tIARC List of Classifications.  This is in Excel format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to import libraries that is going to be needed for this exercise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "In this part, I will read each data set in here and then work on cleaning them one by one and eventually have\n",
    "the parts that I will be using in my final database clean and ready.\n",
    "\n",
    "I have more experince working with Excel files in Python so I start with that. \n",
    "First read it here, so I could have a better understanding of what I am working with.\n",
    "\n",
    "Then I will work on text files, that apears to be fixed width files.\n",
    "\n",
    "I will work on the XML files at the end of this part, since I think they are the challenging ones for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#First reading it using Pandas (personal perefrence)\n",
    "#After taking look at it noticed the header are on the second row (indexing starts at 0 in Python)\n",
    "\n",
    "iarc = pd.read_excel('input\\List_of_Classifications.xls', header = 1)\n",
    "\n",
    "#The only two columns that I will be using from this table are CAS # and Agent\n",
    "\n",
    "classification_list = iarc[[\"CAS No.\", \"Agent\"]]\n",
    "\n",
    "#There are lots of NaNs in the file and since we want to have the CAS # for \n",
    "#all the chemicals that we are using in the final database, we could just drop them\n",
    "\n",
    "classification_list.dropna(subset = ['CAS No.'], inplace = True, axis = 0)\n",
    "\n",
    "#The index has to be reset when you drop certain rows\n",
    "#Indexing is not that important in the end since I want to save these data sets without their indexing\n",
    "\n",
    "classification_list.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#Checking the new data set there are strange characters in the CAS # and agent names\n",
    "#Since there are html tags in some rows, we could use BeautifulSoup to get rid of them\n",
    "\n",
    "classification_list['Agent'] = [BeautifulSoup(text, 'lxml').get_text() for text in classification_list['Agent']]\n",
    "classification_list['CAS No.'] = [BeautifulSoup(text, 'lxml').get_text() for text in classification_list['CAS No.']]\n",
    "\n",
    "#There are some unwanted characters as well, so we try to clean them up too\n",
    "\n",
    "classification_list['CAS No.'] = classification_list['CAS No.'].str.split(\"\\n\").str[0]\n",
    "classification_list['CAS No.'] = classification_list['CAS No.'].str.split(\"[\").str[0]\n",
    "\n",
    "#Adding a column for source\n",
    "\n",
    "classification_list['Source_IARC'] = 'IARC'\n",
    "\n",
    "#Taking a look at this, looks clean and ready to use\n",
    "\n",
    "classification_list.to_excel('IARC_clean.xls', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next step, I want to read in the text file.\n",
    "In this data set we have several text files that are fixed width data sets so we read them using that.\n",
    "After carful consideration of each file and reading the pdf file I tried to go about doing this in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#First file to read in is the Chemcas.txt\n",
    "#Taking a look at it I figured that by specifying the colspecs we could get the columns\n",
    "#Using the pdf file, we used the correct naming for each column\n",
    "\n",
    "chemCas = pd.read_fwf('input/CHEMCAS.TXT', colspecs=[(0, 6), (7, None)], header = None)\n",
    "chemCas.columns = ['PC_Code', 'CAS_NR']\n",
    "\n",
    "#The scond file is Chemname.txt\n",
    "#This one was the tricky one since the pdf did not mention the naming for each column specifically\n",
    "#but I came up with something that I could understand\n",
    "#I skiped the first 4 rows because it was difficult to understand them\n",
    "\n",
    "chemName = pd.read_fwf('input/CHEMNAME.TXT', header = None, colspecs=[(0,6),(6,7),(7,10),(10, 12), (12, None)], skiprows = 4)\n",
    "\n",
    "#Getting rid of the whitespaces in the following columns\n",
    "chemName[3].map(str)\n",
    "chemName[4].map(str)\n",
    "\n",
    "chemName[3].str.strip(' ')\n",
    "chemName[4].str.strip(' ')\n",
    "\n",
    "#Replacing the NaN values with an empty character for the concatenation purposes\n",
    "\n",
    "chemName[3].replace(np.nan, '', inplace = True)\n",
    "\n",
    "#Having both of the columns in column 4\n",
    "\n",
    "chemName[4] = chemName[3] + chemName[4]\n",
    "\n",
    "#I noticed the rows with R in the first column of text file\n",
    "#have the CAS number, so I used the second column that include that part to make a new dataframe for CAS #\n",
    "#I get the ID part as well in order to connect the names to the right CAS #\n",
    "\n",
    "CAS_no = chemName[chemName[1] == 'R'][[0, 4]]\n",
    "\n",
    "#Cleaning parts because it has some strings as well as the CAS # itself\n",
    "\n",
    "CAS_no[4] = CAS_no[4].str.split('No.').str[1]\n",
    "CAS_no[4] = CAS_no[4].str.split('(').str[0]\n",
    "\n",
    "\n",
    "#Getting rid of the whitespace\n",
    "\n",
    "CAS_no[4] = CAS_no[4].str.strip(' ')\n",
    "\n",
    "#Adding header\n",
    "\n",
    "CAS_header = ['ID', 'CAS_NR']\n",
    "CAS_no.columns = CAS_header\n",
    "\n",
    "#I noticed that there are T, C, S values in the second column as well so I just\n",
    "#made a dataframe to record each of them there\n",
    "#The ID is needed as well for the last part\n",
    "\n",
    "t_name = chemName[chemName[1] == 'T'][[0, 4]]\n",
    "t_name.columns = ['ID', 'T_name']\n",
    "c_name = chemName[chemName[1] == 'C'][[0, 4]]\n",
    "c_name.columns = ['ID', 'C_name']\n",
    "\n",
    "#Based on what was mentioned in the pdf file, the rows with S are Systematic names\n",
    "\n",
    "s_name = chemName[chemName[1] == 'S'][[0, 4]]\n",
    "s_name.columns = ['ID', 'systematic_name']\n",
    "\n",
    "#In order to make it easire for myself, I replaced nan values in second columns with string 'Na'\n",
    "#and then record them in another dataframe\n",
    "#It was mentionded in the pdf file that this part is common name \n",
    "#so that's what I called this column\n",
    "\n",
    "chemName[1].replace(np.nan, 'Na', inplace = True)\n",
    "names = chemName[chemName[1] == 'Na'][[0, 4]]\n",
    "names_header = ['ID', 'Common_name']\n",
    "names.columns = names_header\n",
    "\n",
    "#Other text files such as Company.txt, Formula.txt, Prodtype.txt and Product.txt were read into here as well\n",
    "#I used pdf file as reference for the columns name\n",
    "\n",
    "comName = pd.read_fwf('input/COMPANY.TXT', header = None, colspecs=[(0,6),(7,66),(66, 126), (126, 176), (176, 226), (226, 228), (228, 233), (233, 273), (273, 283)])\n",
    "comName.columns = ['CO_NR', 'CO_Name', 'CO_Name2', 'CO_Street', 'CO_City', 'CO_State', 'CO_Zip', 'CO_Contact', 'CO_Phone']\n",
    "\n",
    "formula = pd.read_fwf('input/FORMULA.TXT', header = None, colspecs=[(0,11),(11,17),(17, None)])\n",
    "formula.columns = ['REG_NR', 'PC_Code', 'PC_PCT']\n",
    "\n",
    "prodtype = pd.read_fwf('input/PRODTYPE.TXT', header = None, colspecs=[(0,11),(11, None)])\n",
    "prodtype.columns = ['REG_NR', 'Type_Code']\n",
    "\n",
    "product = pd.read_fwf('input/PRODUCT.TXT', header = None, colspecs=[(0,11),(11,13),(13, 14), (14, 22), (22, 30), (30, 32), (32, 102), (102, 103), (103, 105), (105, None)])\n",
    "product.columns = ['REG_NR', 'Form_Code', 'TOX_Code', 'APPR_Date', 'CAN_Date', 'CT_Date', 'Prod_Name', 'RUP_Flag', 'PM_Code', 'COND_Flag']\n",
    "\n",
    "#Going through the dataframes, I think common and systematic names as well as CAS # are the most important columns\n",
    "#so I only use them in the final datafram from this source\n",
    "#I used ID for joining since that was the only common part between them\n",
    "\n",
    "ppis = names.join(CAS_no.set_index('ID'), on = 'ID')\n",
    "ppis.dropna(subset = ['CAS_NR'], inplace = True)\n",
    "ppis.reset_index(drop=True, inplace=True)\n",
    "ppis = ppis.join(t_name.set_index('ID'), on = 'ID')\n",
    "ppis = ppis.join(c_name.set_index('ID'), on = 'ID')\n",
    "ppis = ppis.join(s_name.set_index('ID'), on = 'ID')\n",
    "\n",
    "#There were couple of cases that had / in instead of - in the CAS #\n",
    "#I was trying to target those but I could not\n",
    "#Then I figure it out that they are not actually in the dataset here\n",
    "#so something was happening to them when I was saving them as CSV file\n",
    "#so I decided on saving them as excel file and see what happens\n",
    "#fortunately, the problem was fixed by doing that\n",
    "\n",
    "#My attepts to replace the non existing forward slash\n",
    "\n",
    "#ppis['CAS_NR'] = ppis['CAS_NR'].map(str)\n",
    "#ppis[~ppis['CAS_NR'].str.contains('/')]\n",
    "#ppis['CAS_NR'].replace('/', np.nan, inplace = True)\n",
    "#ppis.dropna(subset=['CAS_NR'], inplace = True)\n",
    "\n",
    "\n",
    "#Correcting for the spaces in the middle of the names\n",
    "\n",
    "ppis['Common_name'].replace('-\\s+', '-', regex=True, inplace = True)\n",
    "ppis['systematic_name'].replace('-\\s+', '-', regex=True, inplace = True)\n",
    "\n",
    "\n",
    "#Adding source column \n",
    "\n",
    "ppis['Source_PPIS'] = 'PPIS'\n",
    "\n",
    "#Final dataframe from PPIS source\n",
    "\n",
    "ppis.to_excel('PPIS_clean.xls', index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For me the tricky part was working with XML files.\n",
    "I have never actually worked with this type especially when it has many attributes.\n",
    "After doing research and try some different methods I came up with the following code.\n",
    "It might not be most efficient way to do this but it is the way I found that could actually give me what I am looking for.\n",
    "\n",
    "The first data set that I am working with in XML format is ChemId.xml\n",
    "I looked for attributes that I thought could be useful for the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Opening the file and saving it into chemId using BeatifulSoup\n",
    "\n",
    "with open(\"input/chemid.xml\") as chemid:\n",
    "    chemId = BeautifulSoup(chemid, 'xml')\n",
    "    \n",
    "#Creating a list that could be used to append the values to\n",
    "\n",
    "chemID_data = []\n",
    "\n",
    "#Looping through the chemId and finding the chamical tags that are used for each chemical compound (Used dtd file as reference)\n",
    "\n",
    "for element in chemId.find_all('Chemical'):\n",
    "    \n",
    "    #I thought this part could be useful so record this value\n",
    "    \n",
    "    if element.find('DescriptorName') == None:\n",
    "        chemID_data.append(None) #There was a problem with None values so I had to make a condition for that\n",
    "    else:\n",
    "        chemID_data.append((element.find('DescriptorName')).text)\n",
    "        \n",
    "    #Systematic name was another attribute that I found useful for the final result    \n",
    "        \n",
    "    if element.find('SystematicName') == None:\n",
    "        chemID_data.append(None) #Solving the None value problem\n",
    "    else:\n",
    "        chemID_data.append((element.find('SystematicName')).text)\n",
    "        \n",
    "    #Definetly the CAS # is important and has to be recorded\n",
    "    \n",
    "    if element.find('CASRegistryNumber') == None:\n",
    "        chemID_data.append(None)\n",
    "    else:\n",
    "        chemID_data.append((element.find('CASRegistryNumber')).text)\n",
    "    \n",
    "    #Thought having the source could be nice\n",
    "        \n",
    "    if element.find('SourceList') == None:\n",
    "        chemID_data.append(None)\n",
    "    else:\n",
    "        chemID_data.append((element.find('SourceList')).text)\n",
    "        \n",
    "#We need to make it more look like a table since it is just a list of data\n",
    "#In order to do that we reshape it to 83 rows and 4 columns\n",
    "\n",
    "#len(chemID_data)\n",
    "\n",
    "chemID_data = np.reshape(chemID_data, (83, 4))\n",
    "\n",
    "#Convert it to dataframe because I am trying to have all the data set in this format which is\n",
    "#very straightforward to work with\n",
    "\n",
    "chemID_data = pd.DataFrame(chemID_data)\n",
    "\n",
    "#Header for each columns\n",
    "\n",
    "chemID_data.columns = ['DescriptorName', 'SystematicName', 'CASRegistryNumber', 'SourceList']\n",
    "\n",
    "#There was a case that CAS # had PubMed,... next to it and I tried to correct that data\n",
    "\n",
    "chemID_data['CASRegistryNumber'] = chemID_data['CASRegistryNumber'].str.split('PubMed').str[0]\n",
    "chemID_data['SystematicName'].replace('INDEX NAME NOT YET ASSIGNEDNLM', np.nan, inplace = True)\n",
    "chemID_data.dropna(subset=['SystematicName'], inplace = True)\n",
    "chemID_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#Adding the source column\n",
    "\n",
    "chemID_data['Source_ChemID'] = 'ChemID'\n",
    "\n",
    "#Final result of ChemId data set\n",
    "\n",
    "chemID_data.to_excel('ChemID_clean.xls', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 4th data set that I will be reading into here is Mesh\n",
    "This one is in XML format as well so I will try to do the same thing that I did for ChemId data set for this one\n",
    "I will be looking for the data that could be useful for the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Opening the MeSHSupplemental.xml and saving it in Mesh using BeautifulSoup\n",
    "\n",
    "with open(\"input/MeSHSupplemental.xml\") as Mesh:\n",
    "    Mesh = BeautifulSoup(Mesh, 'xml')\n",
    "    \n",
    "#Creating an empty list fot recording the values that I will be looking for\n",
    "\n",
    "Mesh_data = []\n",
    "\n",
    "#Looping through the Mesh\n",
    "#After looking into the data I found that each chemical records are called SupplementaRecord (dtd file was not really helpful in here)\n",
    "#that is why I found that one\n",
    "\n",
    "for element in Mesh.find_all('SupplementalRecord'):\n",
    "    \n",
    "    #Concept name was including the name so I recorded them\n",
    "    \n",
    "    if (element.find('ConceptName')).text == None: #Considering the None case\n",
    "        Mesh_data.append(None)\n",
    "    else:\n",
    "        Mesh_data.append((element.find('ConceptName')).text)\n",
    "    \n",
    "    #CAS name is another name that might be helpful\n",
    "    \n",
    "    if (element.find('CASN1Name')) == None:#Considering None case\n",
    "        Mesh_data.append(None)\n",
    "    else:\n",
    "        Mesh_data.append((element.find('CASN1Name')).text)\n",
    "        \n",
    "    #Recording the CAS # for each row    \n",
    "    if (element.find('RegistryNumber')) == None:\n",
    "        Mesh_data.append(None)\n",
    "    else:\n",
    "        Mesh_data.append((element.find('RegistryNumber')).text)\n",
    "    \n",
    "        \n",
    "    #Having the source coule be nice as well\n",
    "    if (element.find('Source')) == None:\n",
    "        Mesh_data.append(None)\n",
    "    else:\n",
    "        Mesh_data.append((element.find('Source')).text)\n",
    "        \n",
    "    \n",
    "#len(Mesh_data)\n",
    "\n",
    "#We need to reshape the list in order to have something closer to a table\n",
    "\n",
    "Mesh_data = np.reshape(Mesh_data, (79, 4))\n",
    "\n",
    "#Converting to dataframe for future uses\n",
    "\n",
    "Mesh_data = pd.DataFrame(Mesh_data)\n",
    "\n",
    "#Setting the column names\n",
    "\n",
    "Mesh_data.columns = ['ConceptName', 'CASN1Name', 'RegistryNumber', 'Source']\n",
    "\n",
    "#Getting rid of the \\n characters in the names\n",
    "\n",
    "Mesh_data['ConceptName'] = Mesh_data['ConceptName'].str.split('\\n').str[1]\n",
    "\n",
    "#Replacing 0 with NaN for easier dropping\n",
    "\n",
    "Mesh_data['RegistryNumber'].replace('0', np.nan, inplace = True)\n",
    "Mesh_data = Mesh_data[Mesh_data['RegistryNumber'].str.contains(\"A|C|E|F|O|Y|\\|/\") == False]\n",
    "Mesh_data.dropna(subset=['RegistryNumber'], inplace = True)\n",
    "\n",
    "#We have to reset the index when we drop values from dataframe\n",
    "\n",
    "Mesh_data.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#Adding the source column\n",
    "\n",
    "Mesh_data['Source_MeSH'] = 'MeSH'\n",
    "\n",
    "#Final result of the Mesh dataset\n",
    "\n",
    "Mesh_data.to_excel('Mesh_clean.xls', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our datasets, we could start working on generating files that only contain the columns that we are\n",
    "looking for and get rid of the duplicates.\n",
    "In order to do that I will be generating several tables.\n",
    "     1. containg different names for each chemical\n",
    "     2. containg the CAS # for each chemical\n",
    "     3. containing sources and CAS # for each chemical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For name table we need to have common name and systematic names for each chemical\n",
    "#We do not want any duplicates\n",
    "#We join the tables on CAS numbers and it is an outer join since we want to have everything\n",
    "#The only data set that does not have to be an outer join is the IARC\n",
    "#We only want the chemical compounds that are already in the data, so we get a left join\n",
    "\n",
    "names_final = chemID_data.join(ppis.set_index(['systematic_name', 'CAS_NR', 'Common_name']), \n",
    "                         how ='outer', on = ['SystematicName', 'CASRegistryNumber', \n",
    "                        'DescriptorName']).join(Mesh_data.set_index(['CASN1Name', 'RegistryNumber', 'ConceptName']), \n",
    "                         how = 'outer', on = ['SystematicName', 'CASRegistryNumber','DescriptorName']).join(classification_list.set_index(['Agent']),\n",
    "                         how = 'left', on = ['DescriptorName'])[['DescriptorName', 'SystematicName', 'CASRegistryNumber']]\n",
    "\n",
    "#Getting rid of the duplicates\n",
    "\n",
    "names_final.drop_duplicates(subset=['DescriptorName'], inplace=True)\n",
    "names_final.drop_duplicates(subset=['SystematicName'], inplace=True)\n",
    "\n",
    "#names_final = names_final[names_final['CASRegistryNumber'].str.contains(\"A|C|E|F|O|Y|\\|/\") == False]\n",
    "\n",
    "\n",
    "#Reseting the index since we droped couple of rows\n",
    "\n",
    "names_final.reset_index(drop = True, inplace=True)\n",
    "\n",
    "#Getting the CAS # their own file\n",
    "\n",
    "CAS_final = names_final['CASRegistryNumber']\n",
    "CAS_final.to_excel('CAS_final.xls', index = False)\n",
    "\n",
    "#Final result for names \n",
    "names_final.to_excel('names_final.xls', index = False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For source table\n",
    "#We could have a table of CAS # and find in which tables they apear\n",
    "\n",
    "source_final = ppis.join(chemID_data.set_index('CASRegistryNumber'),\n",
    "                      how = 'outer', on = 'CAS_NR').join(Mesh_data.set_index('RegistryNumber'),\n",
    "                      how = 'outer', on = 'CAS_NR').join(classification_list.set_index('CAS No.'), \n",
    "                      how = 'left', on = 'CAS_NR')[['Source_PPIS', 'Source_ChemID', 'Source_MeSH', 'Source_IARC', 'CAS_NR']]\n",
    "\n",
    "#Replacing NaN values with empty space\n",
    "\n",
    "source_final['Source_PPIS'].replace(np.nan, '', inplace = True) \n",
    "source_final['Source_ChemID'].replace(np.nan, '', inplace = True)\n",
    "source_final['Source_IARC'].replace(np.nan, '', inplace = True) \n",
    "source_final['Source_MeSH'].replace(np.nan, '', inplace = True)\n",
    "\n",
    "#Adding the values in columns so we could have all the sources that the chemicals apeared\n",
    "\n",
    "source_final['Source'] = source_final['Source_PPIS'] + ' ' + source_final['Source_ChemID'] + ' ' + source_final['Source_MeSH'] + ' ' + source_final['Source_IARC']\n",
    "\n",
    "#Droping the columns that we are done with\n",
    "\n",
    "source_final.drop(labels=['Source_ChemID', 'Source_IARC', 'Source_MeSH', 'Source_PPIS'], axis = 1, inplace=True)\n",
    "source_final.drop_duplicates(subset=['CAS_NR'], inplace=True)\n",
    "\n",
    "#Only having the values for chemicals that are in CAS_final file\n",
    "\n",
    "source_final = source_final[source_final['CAS_NR'].isin(CAS_final)]\n",
    "\n",
    "#Taking a look at the final result\n",
    "source_final.to_excel('source_final.xls', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database development\n",
    "Now it is time to make the database itself.\n",
    "In order to do that, I will use SQLite3 library from Python since I am working on this on my laptop\n",
    "and it does not require an actual server.\n",
    "I will be using the 3 data set that I generated in the privous steps:\n",
    "the CAS_final, names_final and source_final.\n",
    "I want to generate 3 tables in this database. \n",
    "\n",
    "1. Contains the main chemID as pk that I will be giving to each chemical structure and its CAS number. \n",
    "2. Contains the nameID as pk and chemID as fk and common and systematic name of each chemical.\n",
    "3. Contains the sourceID as pk, nameID and chemID as fk and source of each chemical.\n",
    "\n",
    "The dataset that I am looking to gain from this database has to contain chemID, names and sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "#Reading the CAS_final file in here\n",
    "\n",
    "data = pd.read_excel('CAS_final.xls', header = None)\n",
    "\n",
    "#Generating a database called chemicals\n",
    "#and connecting to it\n",
    "\n",
    "database = sqlite3.connect('output/chemicals.db')\n",
    "\n",
    "#Generatin a cursor\n",
    "\n",
    "c = database.cursor()\n",
    "\n",
    "#Defining a function that create the table if it does not exist\n",
    "#I include the not exist part so it won't generate an error if the table already exists\n",
    "\n",
    "def create_table():\n",
    "    \n",
    "    #I want to have 3 columns, chemID as pk, and CAS_NUMBER\n",
    "    \n",
    "    c.execute('CREATE TABLE IF NOT EXISTS chemicalsID(chemID INT PRIMARY KEY, CAS_NUMBER VARCHAR(30))')\n",
    "    \n",
    "#Defining a function for inserting the data into our first table\n",
    "    \n",
    "def insert_data():\n",
    "    \n",
    "    #I am giving each chemical an ID of integer that is incrementing as I go forward\n",
    "    #Then I just add CAS number from the CAS_final file that is in the ith position\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        value =  'INSERT INTO chemicalsID VALUES('+ str(i)+\",\\'\"+ data[0][i]+ \"\\')\" #I found an easier way that I used for next table\n",
    "        value = str(value)\n",
    "        c.execute(value)\n",
    "    \n",
    "    #Commiting the execution\n",
    "    \n",
    "    database.commit()\n",
    "    \n",
    "    #Closing the cursor\n",
    "    \n",
    "    c.close()\n",
    "    \n",
    "    #Closing the connection \n",
    "    \n",
    "    database.close()\n",
    "    \n",
    "#Calling the functions that we generated above\n",
    "\n",
    "create_table()\n",
    "insert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For second table in our database I will use the names_final file\n",
    "#Reading the file in here\n",
    "\n",
    "names_data = pd.read_excel('names_final.xls')\n",
    "\n",
    "#Connecting to the chemicals database that we generated in the previous step\n",
    "\n",
    "database = sqlite3.connect('output/chemicals.db')\n",
    "\n",
    "#Generating a cursor\n",
    "\n",
    "c = database.cursor()\n",
    "\n",
    "#Defining a function for creating the second table\n",
    "#In here I want to have 4 columns\n",
    "#nameID as pk, chemID as fk and common and systematic name for each chemical\n",
    "\n",
    "def create_table():\n",
    "    c.execute('CREATE TABLE IF NOT EXISTS chemicalsName(nameID INT PRIMARY KEY, chemID INT, common_name VARCHAR(100), systematic_name VARCHAR(100), FOREIGN KEY (chemID) REFERENCES chemicalsID(chemID))')\n",
    "    \n",
    "#Defining a function for inserting the data into this table\n",
    "\n",
    "def insert_data():\n",
    "    \n",
    "    for i in range(len(names_data)):\n",
    "        \n",
    "        #Getting the CAS number from the names_data file\n",
    "        \n",
    "        cas_num = names_data['CASRegistryNumber'][i]\n",
    "        \n",
    "        #Finding the chemID of this CAS # from our chemicalsID table\n",
    "        \n",
    "        c.execute(\"SELECT chemID FROM chemicalsID WHERE CAS_NUMBER = \\'\"+ (cas_num)+\"\\'\")\n",
    "        chemID = c.fetchone()[0]\n",
    "        \n",
    "        #Getting the names from our names_data file\n",
    "        \n",
    "        common_name = names_data['DescriptorName'][i]\n",
    "        systematic_name = names_data['SystematicName'][i]\n",
    "        \n",
    "        #Inserting the values into the table\n",
    "        #I found this way easier than the one I used for first table but I kept the other version too\n",
    "        \n",
    "        c.execute(\"INSERT INTO chemicalsName VALUES(?, ?, ?, ?)\", (i, chemID, common_name, systematic_name))\n",
    "    \n",
    "    #comming the execution    \n",
    "    \n",
    "    database.commit()\n",
    "    \n",
    "    #Closing the cursor\n",
    "    \n",
    "    c.close()\n",
    "    \n",
    "    #Closing the connection to database\n",
    "    \n",
    "    database.close()\n",
    "    \n",
    "\n",
    "#Calling the generated functions\n",
    "\n",
    "create_table()\n",
    "insert_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the third table in our database\n",
    "#The source_final file is going to be used\n",
    "\n",
    "#Reading the file in here\n",
    "\n",
    "source_data = pd.read_excel('source_final.xls')\n",
    "\n",
    "#Connecting to the chemicals database\n",
    "\n",
    "database = sqlite3.connect('output/chemicals.db')\n",
    "\n",
    "#Generating the cursor\n",
    "\n",
    "c = database.cursor()\n",
    "\n",
    "#Defining a function for creating the third table\n",
    "#In here I want to have 4 columns\n",
    "#source_ID as pk, nameID and chemID as fk and source for each chemical\n",
    "\n",
    "def create_table():\n",
    "    c.execute('CREATE TABLE IF NOT EXISTS chemicalSource(source_ID INT PRIMARY KEY, chemID INT, nameID INT, source VARCHAR(100), FOREIGN KEY (chemID) REFERENCES chemicalsID(chemID), FOREIGN KEY (nameID) REFERENCES chemicalsName(nameID))')\n",
    "    \n",
    "#Defining a function for inserting the data into this table\n",
    "\n",
    "def insert_data():\n",
    "    for i in range(len(source_data)):\n",
    "        \n",
    "        #Fining the ith CAS # in source_data file\n",
    "        \n",
    "        cas_num = source_data['CAS_NR'][i]\n",
    "        \n",
    "        #Finding the chemID of that CAS # in the chemicalsID table\n",
    "        \n",
    "        c.execute(\"SELECT chemID FROM chemicalsID WHERE CAS_NUMBER = \\'\"+ (cas_num)+\"\\'\")\n",
    "        chemID = c.fetchone()[0]\n",
    "        \n",
    "        #Finding the nameID of that chemID that we found above in chemicalsName table\n",
    "        \n",
    "        c.execute(\"SELECT nameID FROM chemicalsName WHERE chemID = ?\", (chemID,))\n",
    "        nameID = c.fetchone()[0]\n",
    "        \n",
    "        #Finding the source of ith value in source_data file\n",
    "        \n",
    "        source = source_data['Source'][i]\n",
    "        \n",
    "        #Insering the data into the table\n",
    "        \n",
    "        c.execute(\"INSERT INTO chemicalSource VALUES(?, ?, ?, ?)\", (i, chemID, nameID, source))\n",
    "    \n",
    "    #Commiting the execution\n",
    "    \n",
    "    database.commit()\n",
    "    \n",
    "    #Closing the cursor\n",
    "    \n",
    "    c.close()\n",
    "    \n",
    "    #Closing the connection\n",
    "    \n",
    "    database.close()\n",
    "    \n",
    "#Calling the functions\n",
    "\n",
    "create_table()\n",
    "insert_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our database we could get the main result that we were looking for.\n",
    "We want to execute a query that could get the columns that we were looking for.\n",
    "The CAS #, names and sources of each chemical in a way that they only apear once (I took care of duplicates in the cleaning \n",
    "part but I could have uniqueness as a constraint in generating the tables as well)\n",
    "I order to do that, I will select the columns from joined tables on chemID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connecting to the database\n",
    "\n",
    "database = sqlite3.connect('output/chemicals.db')\n",
    "\n",
    "#Generating a cursor\n",
    "\n",
    "c = database.cursor()\n",
    "\n",
    "#Select statement that has to target CAS_NUMBER, common_name, systematic_name and source\n",
    "#We will join the 3 tables on chemID which is in all the tables\n",
    "\n",
    "c.execute('SELECT CAS_NUMBER, common_name, systematic_name, source FROM chemicalsID join chemicalsName on chemicalsID.chemID = chemicalsName.nameID join chemicalSource on chemicalsID.chemID = chemicalSource.chemID')\n",
    "\n",
    "#Getting all the values and putting them in the final dataset\n",
    "\n",
    "final_dataset = c.fetchall()\n",
    "\n",
    "#closing the cursor\n",
    "\n",
    "c.close()\n",
    "\n",
    "#Closing the connection\n",
    "\n",
    "database.close()\n",
    "\n",
    "#Taking a look at the final result\n",
    "\n",
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the file to a DataFrame for a cleaner look\n",
    "\n",
    "final_dataset = pd.DataFrame(final_dataset)\n",
    "\n",
    "final_dataset.columns = ['CAS_NUM', 'Common_name', 'Systematic_name', 'Sources']\n",
    "\n",
    "#Taking a look at the final data set\n",
    "\n",
    "final_dataset.to_excel('output/final_dataset.xls', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
